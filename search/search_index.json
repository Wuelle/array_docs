{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome This is a documentation of my journey to implementing a (hopefully) fast matrix library. It is less of a blog and more of a diary (meaning its primarily intended for myself to look up stuff in the future) I intend to use it for my deep learning library . There are two main folders within this project: Theory , which contains all the knowledge needed. Implementation , which contains actual implementation details in rust.","title":"Home"},{"location":"#welcome","text":"This is a documentation of my journey to implementing a (hopefully) fast matrix library. It is less of a blog and more of a diary (meaning its primarily intended for myself to look up stuff in the future) I intend to use it for my deep learning library . There are two main folders within this project: Theory , which contains all the knowledge needed. Implementation , which contains actual implementation details in rust.","title":"Welcome"},{"location":"credits/","text":"Image Sources Chapter 1 Von Rjaeschke - Eigenes Werk, CC BY-SA 4.0, https://commons.wikimedia.org Chapter 2","title":"Credits"},{"location":"credits/#image-sources","text":"Chapter 1 Von Rjaeschke - Eigenes Werk, CC BY-SA 4.0, https://commons.wikimedia.org Chapter 2","title":"Image Sources"},{"location":"useful_links/","text":"Links These are some links which i have not looked at yet, but seem pretty useful custom blas implementation convolution speedup deep dive into cpu caches","title":"Links"},{"location":"useful_links/#links","text":"These are some links which i have not looked at yet, but seem pretty useful custom blas implementation convolution speedup deep dive into cpu caches","title":"Links"},{"location":"implementation/benchmarking/","text":"Benchmarking What are we benchmarking? After writing [implementation/initialization.md], i got curious about the performance difference between two implementations of Clone . The naive approach looks like this: 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 impl < T : Clone , const N : usize > Clone for Array < T , N > { fn clone ( & self ) -> Self { let mut cloned : Self ; // Safe because we won't be reading from uninitialized memory. unsafe { cloned = Array :: uninitialized ( self . dim ); } // clone each element (can probably be done faster) for offset in 0 .. self . size () { // safe because offset will never exceed self.size() unsafe { * cloned . _get_mut_unchecked ( offset ) = self . _get_unchecked ( offset ). clone (); } } cloned } } This is obviously pretty inefficient since we are cloning each element from the original and writing it to the destination. It (probably) would be faster to copy all of the original array's heap data and write it in one go. I came up with this implementation: 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 impl < T : Copy , const N : usize > Clone for Array < T , N > { fn clone ( & self ) -> Self { let mut cloned : Self ; // Safe because we won't be reading from uninitialized memory. unsafe { cloned = Array :: uninitialized ( self . dim ); } // Safe because // * T is Copy // * self.ptr is valid for self.size() reads // * cloned.ptr is valid for self.size() writes // * both self and cloned are properly aligned // * self and cloned do not overlap unsafe { std :: ptr :: copy_nonoverlapping ( self . ptr , cloned . ptr , self . size ()); } cloned } } Running the benchmark First, you need to install cargo-criterion using cargo install cargo-criterion . You can then run all benchmarks with cargo criterion . First method These are the mean times for cloning a 10x10x10 array using the naive approach: Criterion actually provides a lot more plots than just this one, but too keep this example simple i decided to focus only on the mean time. Second method These are the benchmark results for the second approach: Interestingly enough, this is ~40ns slower than copying each element seperately. I do not know exactly why this is the case, but i suspect the compiler might be able to optimize away the bloat from the first approach.","title":"Benchmarking"},{"location":"implementation/benchmarking/#benchmarking","text":"","title":"Benchmarking"},{"location":"implementation/benchmarking/#what-are-we-benchmarking","text":"After writing [implementation/initialization.md], i got curious about the performance difference between two implementations of Clone . The naive approach looks like this: 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 impl < T : Clone , const N : usize > Clone for Array < T , N > { fn clone ( & self ) -> Self { let mut cloned : Self ; // Safe because we won't be reading from uninitialized memory. unsafe { cloned = Array :: uninitialized ( self . dim ); } // clone each element (can probably be done faster) for offset in 0 .. self . size () { // safe because offset will never exceed self.size() unsafe { * cloned . _get_mut_unchecked ( offset ) = self . _get_unchecked ( offset ). clone (); } } cloned } } This is obviously pretty inefficient since we are cloning each element from the original and writing it to the destination. It (probably) would be faster to copy all of the original array's heap data and write it in one go. I came up with this implementation: 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 impl < T : Copy , const N : usize > Clone for Array < T , N > { fn clone ( & self ) -> Self { let mut cloned : Self ; // Safe because we won't be reading from uninitialized memory. unsafe { cloned = Array :: uninitialized ( self . dim ); } // Safe because // * T is Copy // * self.ptr is valid for self.size() reads // * cloned.ptr is valid for self.size() writes // * both self and cloned are properly aligned // * self and cloned do not overlap unsafe { std :: ptr :: copy_nonoverlapping ( self . ptr , cloned . ptr , self . size ()); } cloned } }","title":"What are we benchmarking?"},{"location":"implementation/benchmarking/#running-the-benchmark","text":"First, you need to install cargo-criterion using cargo install cargo-criterion . You can then run all benchmarks with cargo criterion .","title":"Running the benchmark"},{"location":"implementation/benchmarking/#first-method","text":"These are the mean times for cloning a 10x10x10 array using the naive approach: Criterion actually provides a lot more plots than just this one, but too keep this example simple i decided to focus only on the mean time.","title":"First method"},{"location":"implementation/benchmarking/#second-method","text":"These are the benchmark results for the second approach: Interestingly enough, this is ~40ns slower than copying each element seperately. I do not know exactly why this is the case, but i suspect the compiler might be able to optimize away the bloat from the first approach.","title":"Second method"},{"location":"implementation/initialization/","text":"Cloning and array Initialization Filling the array Up until now, we had to create arrays using Array :: uninitialized and manually assign values. This is obviously a terrible idea, since its both tedious and unsafe. To at least have a temporary solution for debugging and further development, I created the Array :: fill function which creates an array where every element is the same specified value. Having this function finally allows me to implement traits like Clone which previously was not possible, due to requiring clones of uninitialized memory. Implementing Clone It is rare having to implement the Clone trait manually. Usually prefixing a type definition with #[derive(Clone)] , which recursively calls field.clone on each field, is sufficient. However, since we are working with raw pointers, cloning them would make both arrays point to the same memory address, which obviously isn't what we want. A custom Clone implementation for our array type looks like this: 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 impl < T : Copy , const N : usize > Clone for Array < T , N > { fn clone ( & self ) -> Self { let mut cloned : Self ; // Safe because we won't be reading from uninitialized memory. unsafe { cloned = Array :: uninitialized ( self . dim ); } // Safe because // * T is Copy // * self.ptr is valid for self.size() reads // * cloned.ptr is valid for self.size() writes // * both self and cloned are properly aligned // * self and cloned do not overlap unsafe { std :: ptr :: copy_nonoverlapping ( self . ptr , cloned . ptr , self . size ()); } cloned } } Note that we only implement Clone if T also implements Copy . This trait bound isn't very restrictive since basically all primitive types implement Copy (and the ones that do not, like String , are unlikely to be used inside an array). However, it ensures memory safety because std :: ptr :: copy_nonoverlapping performs a bitwise copy without ensuring Copy . We don't implement Copy on Array because it contains heap data which requires a deep copy ( Clone ). Performance I actually got curious about the performance difference between calling . clone () on each element. See [implentation/benchmarking] for more info. 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 impl < T : Clone , const N : usize > Clone for Array < T , N > { fn clone ( & self ) -> Self { let mut cloned : Self ; // Safe because we won't be reading from uninitialized memory. unsafe { cloned = Array :: uninitialized ( self . dim ); } // clone each element (can probably be done faster) for offset in 0 .. self . size () { // safe because offset will never exceed self.size() unsafe { * cloned . _get_mut_unchecked ( offset ) = self . _get_unchecked ( offset ). clone (); } } cloned } }","title":"Cloning and array Initialization"},{"location":"implementation/initialization/#cloning-and-array-initialization","text":"","title":"Cloning and array Initialization"},{"location":"implementation/initialization/#filling-the-array","text":"Up until now, we had to create arrays using Array :: uninitialized and manually assign values. This is obviously a terrible idea, since its both tedious and unsafe. To at least have a temporary solution for debugging and further development, I created the Array :: fill function which creates an array where every element is the same specified value. Having this function finally allows me to implement traits like Clone which previously was not possible, due to requiring clones of uninitialized memory.","title":"Filling the array"},{"location":"implementation/initialization/#implementing-clone","text":"It is rare having to implement the Clone trait manually. Usually prefixing a type definition with #[derive(Clone)] , which recursively calls field.clone on each field, is sufficient. However, since we are working with raw pointers, cloning them would make both arrays point to the same memory address, which obviously isn't what we want. A custom Clone implementation for our array type looks like this: 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 impl < T : Copy , const N : usize > Clone for Array < T , N > { fn clone ( & self ) -> Self { let mut cloned : Self ; // Safe because we won't be reading from uninitialized memory. unsafe { cloned = Array :: uninitialized ( self . dim ); } // Safe because // * T is Copy // * self.ptr is valid for self.size() reads // * cloned.ptr is valid for self.size() writes // * both self and cloned are properly aligned // * self and cloned do not overlap unsafe { std :: ptr :: copy_nonoverlapping ( self . ptr , cloned . ptr , self . size ()); } cloned } } Note that we only implement Clone if T also implements Copy . This trait bound isn't very restrictive since basically all primitive types implement Copy (and the ones that do not, like String , are unlikely to be used inside an array). However, it ensures memory safety because std :: ptr :: copy_nonoverlapping performs a bitwise copy without ensuring Copy . We don't implement Copy on Array because it contains heap data which requires a deep copy ( Clone ).","title":"Implementing Clone"},{"location":"implementation/initialization/#performance","text":"I actually got curious about the performance difference between calling . clone () on each element. See [implentation/benchmarking] for more info. 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 impl < T : Clone , const N : usize > Clone for Array < T , N > { fn clone ( & self ) -> Self { let mut cloned : Self ; // Safe because we won't be reading from uninitialized memory. unsafe { cloned = Array :: uninitialized ( self . dim ); } // clone each element (can probably be done faster) for offset in 0 .. self . size () { // safe because offset will never exceed self.size() unsafe { * cloned . _get_mut_unchecked ( offset ) = self . _get_unchecked ( offset ). clone (); } } cloned } }","title":"Performance"},{"location":"implementation/multidimensional/","text":"Multidimensional Arrays How it works Rust 1.51 introduced const generics, which allow types that are generic over values instead of types . This allows for efficient multidimensional array implementations. Every array has \\(N\\) dimensions with possibly different sizes. This can be represented using [ usize ; N ] which is a usize- array with \\(N\\) elements. Implementing dimensions this way changes the Array < T > struct definition to the following: 3 4 5 6 7 8 9 10 11 /// A n-dimensional array pub struct Array < T , const N : usize > { /// Pointer to the first element ptr : * mut T , /// Number of bytes between two elements stride : usize , /// Axis sizes dim : [ usize ; N ], } Indexing Having arrays be n-dimensional means that to access a specific element, we need \\(N\\) indices. I decided to keep the old Array :: get method but rename it to Array :: _get and make it private. The new Array :: get takes an [ usize ; N ] and converts it to a one dimensional offset using Array :: _get_internal_ix , then uses that offset to get a reference to the element using Array :: _get . Problems using const generics One thing I haven't quite figured out yet is how to implement an Array :: reshape function, which converts Array < T , N > into Array < T , M > . This seems quite simple at first (no data needs to be moved) until you realize const generics are constant. (Who would have thought?), meaning you can't just change the value of \\(N\\) at runtime, forcing you to allocate an entirely new array. It is probably possible to change N using unsafe code, but it won't be pretty. Other stuff i did I added lots of documentation and unit tests. The crate actually requires you to document every publicly exposed object and comment every unsafe block now.","title":"Multidimensional Arrays"},{"location":"implementation/multidimensional/#multidimensional-arrays","text":"","title":"Multidimensional Arrays"},{"location":"implementation/multidimensional/#how-it-works","text":"Rust 1.51 introduced const generics, which allow types that are generic over values instead of types . This allows for efficient multidimensional array implementations. Every array has \\(N\\) dimensions with possibly different sizes. This can be represented using [ usize ; N ] which is a usize- array with \\(N\\) elements. Implementing dimensions this way changes the Array < T > struct definition to the following: 3 4 5 6 7 8 9 10 11 /// A n-dimensional array pub struct Array < T , const N : usize > { /// Pointer to the first element ptr : * mut T , /// Number of bytes between two elements stride : usize , /// Axis sizes dim : [ usize ; N ], }","title":"How it works"},{"location":"implementation/multidimensional/#indexing","text":"Having arrays be n-dimensional means that to access a specific element, we need \\(N\\) indices. I decided to keep the old Array :: get method but rename it to Array :: _get and make it private. The new Array :: get takes an [ usize ; N ] and converts it to a one dimensional offset using Array :: _get_internal_ix , then uses that offset to get a reference to the element using Array :: _get .","title":"Indexing"},{"location":"implementation/multidimensional/#problems-using-const-generics","text":"One thing I haven't quite figured out yet is how to implement an Array :: reshape function, which converts Array < T , N > into Array < T , M > . This seems quite simple at first (no data needs to be moved) until you realize const generics are constant. (Who would have thought?), meaning you can't just change the value of \\(N\\) at runtime, forcing you to allocate an entirely new array. It is probably possible to change N using unsafe code, but it won't be pretty.","title":"Problems using const generics"},{"location":"implementation/multidimensional/#other-stuff-i-did","text":"I added lots of documentation and unit tests. The crate actually requires you to document every publicly exposed object and comment every unsafe block now.","title":"Other stuff i did"},{"location":"implementation/onedimensional/","text":"One-dimensional Array Prerequisites This Implementation Chapter requires understanding of the following Theory Chapters: What is an array? How are arrays stored in memory? Memory Alignment Dependencies Since we are going to be working with raw memory, we need to be able to allocate specific amounts of heap memory. In rust, this can be done by using the std :: alloc crate. 1 use std :: alloc :: { alloc , dealloc , Layout }; The Array Struct To keep it simple, we will constrain our array to a single dimension for now (effectively making it a vector). 3 4 5 6 7 8 9 10 11 /// A one dimensional vector pub struct Array < T > { /// Pointer to the first element ptr : * mut T , /// Number of bytes between two elements stride : usize , /// Number of elements within the vector dim : usize , } Usually, rust will take care of allocating and deallocating memory as needed. In this case however, we need to do it ourselves. To do this, we need to do two things: Define a Constructor (to allocate memory) Define a Destructor (to deallocate memory) Allocation Allocating memory is actually pretty simple. We just need to tell the compiler How much memory to allocate How to align the allocated memory 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 impl < T > Array < T > { /// Create an uninitialized Array<T> pub fn new ( len : usize ) -> Self { let stride = std :: mem :: size_of :: < T > (); let ptr = unsafe { let layout = Layout :: from_size_align_unchecked ( len , stride ); alloc ( layout ) as * mut T }; Self { ptr : ptr , stride : stride , dim : len , } } } Using Layout :: from_size_align_unchecked is unsafe, but provides perfomance benefits over Layout :: from_size_align because we skip runtime checks. Not sure yet if i will keep this. Deallocation To control memory deallocation, we need to implement the Drop trait for Array . This trait only contains the Drop :: drop function, which is called when the value goes out of scope. My implementation looks like this: 62 63 64 65 66 67 68 69 70 71 impl < T > Drop for Array < T > { fn drop ( & mut self ) { unsafe { dealloc ( self . ptr as * mut u8 , Layout :: from_size_align_unchecked ( self . dim , self . stride ), ) }; } } Reading and Writing Being able to allocate memory is nice, but it's also completely useless as long as we can't read from it or write to it. Warning After calling alloc ( layout ) , the memory is not initialized. This means that reading from it will cause undefined behaviour! When adding to a pointer in rust, it will be incremented with a stepsize equal to the size of the element it is pointing to. This is convenient, because it means we dont actually have to multiply ix with self . stride . 28 29 30 31 32 33 34 35 36 37 /// Get a immutable reference to an object within the array pub fn get ( & self , ix : usize ) -> Option <& T > { if ix < self . dim { unsafe { Some ( &* self . ptr . add ( ix )) } } else { None } } and the same for a mutable reference: 44 45 46 47 48 49 50 51 52 53 /// Get a mutable reference to an object stored within the array pub fn get_mut ( & mut self , ix : usize ) -> Option <& mut T > { if ix < self . dim { unsafe { Some ( & mut * self . ptr . add ( ix )) } } else { None } } I also implemented Array :: get_unchecked and Array :: get_mut_unchecked which simply skip the runtime boundary checks.","title":"One-dimensional Array"},{"location":"implementation/onedimensional/#one-dimensional-array","text":"","title":"One-dimensional Array"},{"location":"implementation/onedimensional/#prerequisites","text":"This Implementation Chapter requires understanding of the following Theory Chapters: What is an array? How are arrays stored in memory? Memory Alignment","title":"Prerequisites"},{"location":"implementation/onedimensional/#dependencies","text":"Since we are going to be working with raw memory, we need to be able to allocate specific amounts of heap memory. In rust, this can be done by using the std :: alloc crate. 1 use std :: alloc :: { alloc , dealloc , Layout };","title":"Dependencies"},{"location":"implementation/onedimensional/#the-array-struct","text":"To keep it simple, we will constrain our array to a single dimension for now (effectively making it a vector). 3 4 5 6 7 8 9 10 11 /// A one dimensional vector pub struct Array < T > { /// Pointer to the first element ptr : * mut T , /// Number of bytes between two elements stride : usize , /// Number of elements within the vector dim : usize , } Usually, rust will take care of allocating and deallocating memory as needed. In this case however, we need to do it ourselves. To do this, we need to do two things: Define a Constructor (to allocate memory) Define a Destructor (to deallocate memory)","title":"The Array Struct"},{"location":"implementation/onedimensional/#allocation","text":"Allocating memory is actually pretty simple. We just need to tell the compiler How much memory to allocate How to align the allocated memory 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 impl < T > Array < T > { /// Create an uninitialized Array<T> pub fn new ( len : usize ) -> Self { let stride = std :: mem :: size_of :: < T > (); let ptr = unsafe { let layout = Layout :: from_size_align_unchecked ( len , stride ); alloc ( layout ) as * mut T }; Self { ptr : ptr , stride : stride , dim : len , } } } Using Layout :: from_size_align_unchecked is unsafe, but provides perfomance benefits over Layout :: from_size_align because we skip runtime checks. Not sure yet if i will keep this.","title":"Allocation"},{"location":"implementation/onedimensional/#deallocation","text":"To control memory deallocation, we need to implement the Drop trait for Array . This trait only contains the Drop :: drop function, which is called when the value goes out of scope. My implementation looks like this: 62 63 64 65 66 67 68 69 70 71 impl < T > Drop for Array < T > { fn drop ( & mut self ) { unsafe { dealloc ( self . ptr as * mut u8 , Layout :: from_size_align_unchecked ( self . dim , self . stride ), ) }; } }","title":"Deallocation"},{"location":"implementation/onedimensional/#reading-and-writing","text":"Being able to allocate memory is nice, but it's also completely useless as long as we can't read from it or write to it. Warning After calling alloc ( layout ) , the memory is not initialized. This means that reading from it will cause undefined behaviour! When adding to a pointer in rust, it will be incremented with a stepsize equal to the size of the element it is pointing to. This is convenient, because it means we dont actually have to multiply ix with self . stride . 28 29 30 31 32 33 34 35 36 37 /// Get a immutable reference to an object within the array pub fn get ( & self , ix : usize ) -> Option <& T > { if ix < self . dim { unsafe { Some ( &* self . ptr . add ( ix )) } } else { None } } and the same for a mutable reference: 44 45 46 47 48 49 50 51 52 53 /// Get a mutable reference to an object stored within the array pub fn get_mut ( & mut self , ix : usize ) -> Option <& mut T > { if ix < self . dim { unsafe { Some ( & mut * self . ptr . add ( ix )) } } else { None } } I also implemented Array :: get_unchecked and Array :: get_mut_unchecked which simply skip the runtime boundary checks.","title":"Reading and Writing"},{"location":"theory/Chapter1/","text":"What is an array? An array is a multidimensional collection of objects. An array of size \\(m\\times{n}\\) has m rows and n columns.","title":"What is an array?"},{"location":"theory/Chapter1/#what-is-an-array","text":"An array is a multidimensional collection of objects. An array of size \\(m\\times{n}\\) has m rows and n columns.","title":"What is an array?"},{"location":"theory/Chapter2/","text":"How are arrays stored in memory? As mentioned before, arrays are multidimensional. Since computer memory is one-dimensional, we need to flatten the matrix. This is where strides come into play. A stride is the number of bytes between elements in memory. The array stride is usually equal to the element size. (Unless the matrix is padded) This means that a minimal array implementation contains the following data: The address of the first element The stride of the array The dimensions of the array By knowing these three things, we can computer the address of any given index. There are mainly two different ways to project a n-dimensional array into 1-dimensional space. Row-major Ordering (or \"c\" Ordering) Concatenate all the rows together. This is the common approach for high-level languages. It means that for an array \\(A\\) of size \\(m\\times{n}\\) with a stride of 1, the elements \\(A_{i, j}\\) and \\(A_{i, j+1}\\) are 1 byte apart and the elements \\(A_{i, j}\\) and \\(A_{i+1, j}\\) are n bytes apart. Notable languages that use Row-major Ordering are C, C++ and Java. Column-major Ordering (or \"f\" Ordering) Concatenate all columns together. This approach seems to be less popular. It means that for an array \\(A\\) of size \\(m\\times{n}\\) with a stride of 1, the elements \\(A_{i, j}\\) and \\(A_{i+1, j}\\) are 1 byte apart and the elements \\(A_{i, j}\\) and \\(A_{i, j+1}\\) are m bytes apart. Notable languages that use Row-major Ordering are Fortran and Julia There is no difference in performance between the two. Both provide element lookup at \\(O(1)\\) . The mapping you choose only affects implementation details, i.e you should always traverse the array row by row if you use Row-major Ordering.","title":"How are arrays stored in memory?"},{"location":"theory/Chapter2/#how-are-arrays-stored-in-memory","text":"As mentioned before, arrays are multidimensional. Since computer memory is one-dimensional, we need to flatten the matrix. This is where strides come into play. A stride is the number of bytes between elements in memory. The array stride is usually equal to the element size. (Unless the matrix is padded) This means that a minimal array implementation contains the following data: The address of the first element The stride of the array The dimensions of the array By knowing these three things, we can computer the address of any given index. There are mainly two different ways to project a n-dimensional array into 1-dimensional space.","title":"How are arrays stored in memory?"},{"location":"theory/Chapter2/#row-major-ordering-or-c-ordering","text":"Concatenate all the rows together. This is the common approach for high-level languages. It means that for an array \\(A\\) of size \\(m\\times{n}\\) with a stride of 1, the elements \\(A_{i, j}\\) and \\(A_{i, j+1}\\) are 1 byte apart and the elements \\(A_{i, j}\\) and \\(A_{i+1, j}\\) are n bytes apart. Notable languages that use Row-major Ordering are C, C++ and Java.","title":"Row-major Ordering (or \"c\" Ordering)"},{"location":"theory/Chapter2/#column-major-ordering-or-f-ordering","text":"Concatenate all columns together. This approach seems to be less popular. It means that for an array \\(A\\) of size \\(m\\times{n}\\) with a stride of 1, the elements \\(A_{i, j}\\) and \\(A_{i+1, j}\\) are 1 byte apart and the elements \\(A_{i, j}\\) and \\(A_{i, j+1}\\) are m bytes apart. Notable languages that use Row-major Ordering are Fortran and Julia There is no difference in performance between the two. Both provide element lookup at \\(O(1)\\) . The mapping you choose only affects implementation details, i.e you should always traverse the array row by row if you use Row-major Ordering.","title":"Column-major Ordering (or \"f\" Ordering)"},{"location":"theory/Chapter3/","text":"Memory Alignment What is a Word? A word is defined as the amount of data the processor can load into its registers with a single instruction. The CPU cannot load data from every memory address. Instead, it can only load words starting at an address in memory that is a word boundary. The size of a word is highly dependent on the CPU Architecture, with 32 and 64bit being the most common in modern processors. Because of this limitation, the CPU can only access the memory in chunks, transferring one chunk at a time using either a LDR or a STR Instruction Implications This means that if a value in memory does not start at a word boundary, the processor will have to load more words, resulting in more instructions and lower performance. If the CPU has to perform more than one instruction, the whole process is no longer atomic, meaning that other processes may interfere with the data as it is being read, causing undefined behaviour. Alignment A value is aligned if it starts at a memory address that is a multiple of the processors word size. If that is not the case, it is misaligned. Aligning your data is always a tradeoff: If you perfectly align everything you will end up with a lot of padding to fill the empty space to the next word boundary, thereby wasting memory. If you tightly pack the data and don't align anything, you will use less memory but decrease performance. In general, aligning values is to be strongly preferred. While accessing unaligned values will only decrease performance on x86, some ARM chips might fault.","title":"Memory Alignment"},{"location":"theory/Chapter3/#memory-alignment","text":"","title":"Memory Alignment"},{"location":"theory/Chapter3/#what-is-a-word","text":"A word is defined as the amount of data the processor can load into its registers with a single instruction. The CPU cannot load data from every memory address. Instead, it can only load words starting at an address in memory that is a word boundary. The size of a word is highly dependent on the CPU Architecture, with 32 and 64bit being the most common in modern processors. Because of this limitation, the CPU can only access the memory in chunks, transferring one chunk at a time using either a LDR or a STR Instruction","title":"What is a Word?"},{"location":"theory/Chapter3/#implications","text":"This means that if a value in memory does not start at a word boundary, the processor will have to load more words, resulting in more instructions and lower performance. If the CPU has to perform more than one instruction, the whole process is no longer atomic, meaning that other processes may interfere with the data as it is being read, causing undefined behaviour.","title":"Implications"},{"location":"theory/Chapter3/#alignment","text":"A value is aligned if it starts at a memory address that is a multiple of the processors word size. If that is not the case, it is misaligned. Aligning your data is always a tradeoff: If you perfectly align everything you will end up with a lot of padding to fill the empty space to the next word boundary, thereby wasting memory. If you tightly pack the data and don't align anything, you will use less memory but decrease performance. In general, aligning values is to be strongly preferred. While accessing unaligned values will only decrease performance on x86, some ARM chips might fault.","title":"Alignment"}]}